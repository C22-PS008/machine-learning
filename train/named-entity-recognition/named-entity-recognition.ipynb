{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/C22-PS008/machine-learning/blob/main/train/named-entity-recognition/named-entity-recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "735GEiLf0pJX"
      },
      "source": [
        "# Dependency Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8l2gi8_g8fu9"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install tokenizers\n",
        "!pip install datasets\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qm0VEKf5vQYI"
      },
      "outputs": [],
      "source": [
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LRI4dwK04hf"
      },
      "source": [
        "# Log-in and load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cN8JONKDG3d0"
      },
      "outputs": [],
      "source": [
        "!pip install huggingface_hub\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DOcSMD58fvA"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "wikiann_dataset = load_dataset(\"wikiann\",\"id\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMMo80Lt8fvB"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "indonlu_dataset = load_dataset(\"indonlu\",\"nergrit\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77UNAv-wfpSw"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "conll2033_dataset = load_dataset(\"conll2033\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNIfm4dU8fvC"
      },
      "outputs": [],
      "source": [
        "wikiann_label_names=wikiann_dataset[\"train\"].features[f\"ner_tags\"].feature.names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "duYLzjQPBu5z"
      },
      "outputs": [],
      "source": [
        "wikiann_label_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eQQopkd8fvD"
      },
      "outputs": [],
      "source": [
        "indonlu_label_names=indonlu_dataset[\"train\"].features[f\"ner_tags\"].feature.names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRexuZ8nqcS7"
      },
      "outputs": [],
      "source": [
        "indonlu_label_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Lob8j02HNE-"
      },
      "outputs": [],
      "source": [
        "indonlu_feature=indonlu_dataset[\"train\"].features\n",
        "indonlu_feature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJKEDBT4flCY"
      },
      "source": [
        "# Defining Model and Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6WENdBRflCY"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer , TFAutoModelForTokenClassification\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"chanifrusydi/bert-finetuned-ner\",from_tf=True)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXVSudY0beXj"
      },
      "outputs": [],
      "source": [
        "model = TFAutoModelForTokenClassification(\"chanifrusydi/bert-finetuned-ner\")\n",
        "# model.bert.load_state_dict(model.bert.state_dict())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJ7IedWQa2ZJ"
      },
      "outputs": [],
      "source": [
        "indobenchmark/indobert-base-p1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0l1zlNUY7TL"
      },
      "source": [
        "# Tokenize Dataset \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9fG__9uO1LS"
      },
      "outputs": [],
      "source": [
        "tokenizer(\n",
        "    [\"Hello\", \",\", \"this\", \"is\", \"one\", \"sentence\", \"split\", \"into\", \"words\", \".\"],\n",
        "    is_split_into_words=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJoImX1QPEQh"
      },
      "outputs": [],
      "source": [
        "def aligning_labels_with_token(labels, word_id):\n",
        "  new_labels=[]\n",
        "  current_word=None\n",
        "  label=0\n",
        "  for word in word_id:\n",
        "    if word != current_word:\n",
        "      current_word=word\n",
        "      if word is None:\n",
        "        label=-100\n",
        "      else:\n",
        "        label=labels[word]\n",
        "      new_labels.append(label)\n",
        "    elif word is None:\n",
        "      new_labels.append(-100)\n",
        "    else:\n",
        "      label=labels[word]\n",
        "      if label %2==1:\n",
        "        label+=1\n",
        "      new_labels.append(label)\n",
        "  return new_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHJc2mb9GUXX"
      },
      "outputs": [],
      "source": [
        "def align_labels_with_tokens(labels, word_ids):\n",
        "    new_labels = []\n",
        "    current_word = None\n",
        "    for word_id in word_ids:\n",
        "        if word_id != current_word:\n",
        "            # Start of a new word!\n",
        "            current_word = word_id\n",
        "            label = -100 if word_id is None else labels[word_id]\n",
        "            new_labels.append(label)\n",
        "        elif word_id is None:\n",
        "            # Special token\n",
        "            new_labels.append(-100)\n",
        "        else:\n",
        "            # Same word as previous token\n",
        "            label = labels[word_id]\n",
        "            # If the label is B-XXX we change it to I-XXX\n",
        "            if label % 2 == 1:\n",
        "                label += 1\n",
        "            new_labels.append(label)\n",
        "\n",
        "    return new_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlKhFm_9flCZ"
      },
      "outputs": [],
      "source": [
        "inputs=tokenizer(indonlu_dataset[\"train\"][0][\"tokens\"], is_split_into_words=True)\n",
        "labels=indonlu_dataset[\"train\"][0][\"ner_tags\"]\n",
        "word_ids=inputs[\"input_ids\"]\n",
        "word_id_test=inputs.word_ids(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhkYyFGCflCZ"
      },
      "outputs": [],
      "source": [
        "def tokenizing_and_labeling(examples):\n",
        "  tokenized_input=tokenizer(examples[\"tokens\"],truncation=True, is_split_into_words=True)\n",
        "  all_label=examples[\"ner_tags\"]\n",
        "  new_labels=[]\n",
        "  for i,labels in enumerate(all_label):\n",
        "    new_labels.append(aligning_labels_with_token(labels,tokenized_input.word_ids(i)))\n",
        "  tokenized_input[\"labels\"]=new_labels\n",
        "  return tokenized_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KeBOkFg3rO9"
      },
      "outputs": [],
      "source": [
        "tokenized_indonlu_dataset=indonlu_dataset.map(tokenizing_and_labeling,batched=True,remove_columns=indonlu_dataset[\"train\"].column_names,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqVAlQCcflCZ"
      },
      "outputs": [],
      "source": [
        "tokenized_dataset=tokenized_indonlu_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRi2_nshLvrc"
      },
      "outputs": [],
      "source": [
        "for i in range(len(tokenized_dataset[\"train\"])):\n",
        "  for j in range( len(tokenized_dataset[\"train\"][i]['labels'] )):\n",
        "    if isinstance([\"train\"][i]['labels'][j], str)==True:\n",
        "      print(f\"found on %d %d\",i,j)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPI3sOyFOcyT"
      },
      "outputs": [],
      "source": [
        "for i in range(len(tokenized_dataset[\"validation\"])):\n",
        "  for j in range( len(tokenized_dataset\"validation\"][i]['labels'] )):\n",
        "    if isinstance(tokenized_dataset[\"validation\"][i]['labels'][j], str)==True:\n",
        "      print(f\"found on %d %d\",i,j)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "at_jhx97OkYT"
      },
      "outputs": [],
      "source": [
        "for i in range(len(tokenized_dataset[\"test\"])):\n",
        "  for j in range( len(tokenized_dataset[\"test\"][i]['labels'] )):\n",
        "    if isinstance(tokenized_dataset[\"test\"][i]['labels'][j], str)==True:\n",
        "      print(f\"found on %d %d\",i,j)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhjK7VaO1xKi"
      },
      "source": [
        "# Pytorch Dataset Preprocessing and loading into the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4OPuFJNL9aY"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYXgAUZ7rmgt"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "torch_train_dataset = DataLoader(tokenize_indonlu_dataset[\"train\"],shuffle=True,collate_fn=pytorch_data_collator,\n",
        "    batch_size=16,\n",
        ")\n",
        "torch_validation_dataset = DataLoader(\n",
        "    tokenize_indonlu_dataset[\"validation\"], collate_fn=pytorch_data_collator, batch_size=16\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMqvwSnWZyBF"
      },
      "outputs": [],
      "source": [
        "from transformers.keras_callbacks import PushToHubCallback\n",
        "\n",
        "callback = PushToHubCallback(output_dir=\"bert-finetuned-squad\", tokenizer=tokenizer)\n",
        "\n",
        "# We're going to do validation afterwards, so no validation mid-training\n",
        "model.fit(tf_train_dataset, callbacks=[callback], epochs=num_train_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wt3JFAI0c5rO"
      },
      "source": [
        "# Pytorch with Accelerator intialize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpU9SI9BuZ5Y"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdIsxf0BubwY"
      },
      "outputs": [],
      "source": [
        "from accelerate import Accelerator\n",
        "\n",
        "accelerator = Accelerator()\n",
        "model, optimizer, torch_train_dataset, torch_validation_dataset = accelerator.prepare(\n",
        "    model, optimizer, torch_train_dataset, torch_validation_dataset\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kd5f0G_tuedj"
      },
      "outputs": [],
      "source": [
        "from transformers import get_scheduler\n",
        "\n",
        "num_train_epochs = 3\n",
        "num_update_steps_per_epoch = len(torch_train_dataset)\n",
        "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
        "\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPcdTk9GumVu"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import Repository, get_full_repo_name\n",
        "\n",
        "model_name = \"bert-finetuned-ner\"\n",
        "repo_name = get_full_repo_name(model_name)\n",
        "repo_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZL-ttQUHuvHN"
      },
      "outputs": [],
      "source": [
        "output_dir = \"bert-finetuned-ner-accelerate\"\n",
        "repo = Repository(output_dir, clone_from=repo_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "noWVY0mVu2iN"
      },
      "outputs": [],
      "source": [
        "def postprocess(predictions, labels):\n",
        "    predictions = predictions.detach().cpu().clone().numpy()\n",
        "    labels = labels.detach().cpu().clone().numpy()\n",
        "\n",
        "    # Remove ignored index (special tokens) and convert to labels\n",
        "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
        "    true_predictions = [\n",
        "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    return true_labels, true_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHQj7xaru_rX"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "for epoch in range(num_train_epochs):\n",
        "    # Training\n",
        "    model.train()\n",
        "    for batch in torch_train_dataset:\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        accelerator.backward(loss)\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    for batch in torch_validation_dataset:\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch)\n",
        "\n",
        "        predictions = outputs.logits.argmax(dim=-1)\n",
        "        labels = batch[\"labels\"]\n",
        "\n",
        "        # Necessary to pad predictions and labels for being gathered\n",
        "        predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=-100)\n",
        "        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
        "\n",
        "        predictions_gathered = accelerator.gather(predictions)\n",
        "        labels_gathered = accelerator.gather(labels)\n",
        "\n",
        "        true_predictions, true_labels = postprocess(predictions_gathered, labels_gathered)\n",
        "        metric.add_batch(predictions=true_predictions, references=true_labels)\n",
        "\n",
        "    results = metric.compute()\n",
        "    print(\n",
        "        f\"epoch {epoch}:\",\n",
        "        {\n",
        "            key: results[f\"overall_{key}\"]\n",
        "            for key in [\"precision\", \"recall\", \"f1\", \"accuracy\"]\n",
        "        },\n",
        "    )\n",
        "\n",
        "    # Save and upload\n",
        "    accelerator.wait_for_everyone()\n",
        "    unwrapped_model = accelerator.unwrap_model(model)\n",
        "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
        "    if accelerator.is_main_process:\n",
        "        tokenizer.save_pretrained(output_dir)\n",
        "        repo.push_to_hub(\n",
        "            commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kTZqml0fXSQ"
      },
      "source": [
        "# Prepare dataset for Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWmc31JcHpp3"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForTokenClassification\n",
        "\n",
        "data_collator=DataCollatorForTokenClassification(tokenizer=tokenizer,return_tensors=\"tf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxaVsKN_Z8Pf"
      },
      "outputs": [],
      "source": [
        "tf_train_dataset = tokenized_dataset[\"train\"].to_tf_dataset(\n",
        "  columns=['attention_mask','input_ids', 'labels','tokens'],\n",
        "  shuffle=True,\n",
        "  batch_size=16,\n",
        "  collate_fn=data_collator,\n",
        ")\n",
        "tf_validation_dataset=tokenized_dataset[\"validation\"].to_tf_dataset(\n",
        "  columns=['attention_mask','input_ids', 'labels','tokens'],\n",
        "  shuffle=False,\n",
        "  batch_size=16,\n",
        "  collate_fn=data_collator,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6BkEYZvflCb"
      },
      "source": [
        "# Training Using Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YjZdoipbuuI"
      },
      "outputs": [],
      "source": [
        "id2label={str(i): label for i, label in enumerate(label_names)}\n",
        "label2id={v: k for k,v in id2label.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrFlppudflCb"
      },
      "outputs": [],
      "source": [
        "from transformers import create_optimizer\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
        "num_train_epochs=5\n",
        "num_train_steps = len(tf_train_dataset) * num_train_epochs\n",
        "optimizer, schedule = create_optimizer(\n",
        "    init_lr=2e-5,\n",
        "    num_warmup_steps=0,\n",
        "    num_train_steps=num_train_steps,\n",
        "    weight_decay_rate=0.01,\n",
        ")\n",
        "model.bert.load_state_dict(model.bert.state_dict())\n",
        "model.compile(optimizer=optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ko_LPDiLZnzc"
      },
      "outputs": [],
      "source": [
        "from transformers import create_optimizer\n",
        "from transformers.keras_callbacks import PushToHubCallback\n",
        "import tensorflow as tf\n",
        "\n",
        "# The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied\n",
        "# by the total number of epochs. Note that the tf_train_dataset here is a batched tf.data.Dataset,\n",
        "# not the original Hugging Face Dataset, so its len() is already num_samples // batch_size.\n",
        "num_train_epochs = 3\n",
        "num_train_steps = len(tf_train_dataset) * num_train_epochs\n",
        "optimizer, schedule = create_optimizer(\n",
        "    init_lr=2e-5,\n",
        "    num_warmup_steps=0,\n",
        "    num_train_steps=num_train_steps,\n",
        "    weight_decay_rate=0.01,\n",
        ")\n",
        "model.compile(optimizer=optimizer)\n",
        "\n",
        "# Train in mixed-precision float16\n",
        "\n",
        "#tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryWyb5HGflCb"
      },
      "outputs": [],
      "source": [
        "from transformers.keras_callbacks import PushToHubCallback\n",
        "callback=PushToHubCallback(output_dir=\"bert-indonesia-finetuned-ner\", tokenizer=tokenizer)\n",
        "model.fit(tf_train_dataset,validation_dataset=tf_validation_dataset, callbacks=[callback], epochs=num_train_epochs)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "named-entity-recognition.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}